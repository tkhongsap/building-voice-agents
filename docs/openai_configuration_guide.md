# OpenAI-Focused Voice Processing Pipeline Configuration

This guide shows how to configure Task 1.0 Core Voice Processing Pipeline to work primarily with OpenAI services for cost-effective voice agent development.

## üîë Required API Keys

For a minimal OpenAI-focused setup, you only need:

- **OpenAI API Key** - For STT (Whisper) and LLM (GPT-4.1-mini)

## üì± Recommended Models

### STT (Speech-to-Text)
- **Primary**: `whisper-1` (OpenAI Whisper)
- **Cost**: $0.006 per minute of audio
- **Features**: 99+ languages, high accuracy, streaming support

### LLM (Large Language Model) 
- **Primary**: `gpt-4.1-mini` ‚≠ê **RECOMMENDED**
- **Cost**: ~60% cheaper than GPT-4o
- **Features**: Better performance, multimodal capabilities, function calling
- **Alternative**: `gpt-4o-mini` (older but still good)

### TTS (Text-to-Speech)
OpenAI doesn't provide TTS API yet, so choose one:
- **Option 1**: ElevenLabs (requires ElevenLabs API key)
- **Option 2**: Azure TTS (requires Azure subscription)
- **Option 3**: AWS Polly (requires AWS credentials)

### VAD (Voice Activity Detection)
- **Primary**: WebRTC VAD (no API key needed)
- **Alternative**: Silero VAD (local model, no API key needed)

## ‚öôÔ∏è Configuration Examples

### Minimal OpenAI Setup

```python
from components.stt.openai_stt import OpenAISTTProvider, OpenAISTTConfig
from components.llm.openai_llm import OpenAILLMProvider, OpenAILLMConfig
from components.vad.webrtc_vad import WebRTCVADProvider, WebRTCVADConfig
from components.llm.base_llm import LLMModelType

# STT Configuration
stt_config = OpenAISTTConfig(
    api_key="your-openai-api-key",
    model="whisper-1",
    language="en",
    sample_rate=16000
)

# LLM Configuration  
llm_config = OpenAILLMConfig(
    api_key="your-openai-api-key",
    model=LLMModelType.GPT_4_1_MINI,  # Latest and most cost-effective
    temperature=0.7,
    max_tokens=1000
)

# VAD Configuration (no API key needed)
vad_config = WebRTCVADConfig(
    aggressiveness=2,  # 0-3, higher = more aggressive
    sample_rate=16000
)

# Create providers
stt_provider = OpenAISTTProvider(stt_config)
llm_provider = OpenAILLMProvider(llm_config)  
vad_provider = WebRTCVADProvider(vad_config)
```

### Environment Configuration (.env)

```bash
# Required for OpenAI pipeline
OPENAI_API_KEY=sk-proj-your-openai-api-key-here

# Optional for TTS (choose one)
ELEVENLABS_API_KEY=your-elevenlabs-key
# OR
AZURE_SPEECH_KEY=your-azure-key
AZURE_SPEECH_REGION=your-region
# OR  
AWS_ACCESS_KEY_ID=your-aws-key
AWS_SECRET_ACCESS_KEY=your-aws-secret
AWS_REGION=us-west-2
```

## üöÄ Complete Pipeline Setup

```python
from pipeline.audio_pipeline import StreamingAudioPipeline, PipelineConfig, PipelineMode

# Pipeline configuration
pipeline_config = PipelineConfig(
    sample_rate=16000,
    mode=PipelineMode.CONTINUOUS,
    enable_streaming_stt=True,
    enable_streaming_llm=True,
    latency_target_ms=236  # Our performance target
)

# Create the pipeline
pipeline = StreamingAudioPipeline(
    config=pipeline_config,
    stt_provider=stt_provider,      # OpenAI Whisper
    llm_provider=llm_provider,      # OpenAI GPT-4.1-mini
    tts_provider=tts_provider,      # Your chosen TTS provider
    vad_provider=vad_provider       # WebRTC VAD
)

# Initialize and start
await pipeline.initialize()
await pipeline.start()
```

## üí∞ Cost Optimization Tips

### Model Selection
- **GPT-4.1-mini**: Latest, best performance/cost ratio
- **Whisper**: Cost-effective STT, charge per minute of audio
- **WebRTC VAD**: Free, no API costs

### Usage Optimization
```python
# Optimize for cost
llm_config = OpenAILLMConfig(
    model=LLMModelType.GPT_4_1_MINI,
    max_tokens=500,        # Limit response length
    temperature=0.3,       # Lower temperature = more focused responses
    top_p=0.9             # Nucleus sampling for efficiency
)

# Optimize STT usage
stt_config = OpenAISTTConfig(
    model="whisper-1",
    language="en",         # Specify language for better accuracy
    response_format="json" # Structured response
)
```

## üõ°Ô∏è Fallback Configuration

Even with an OpenAI-focused setup, configure fallbacks for production:

```python
from components.error_handling import global_error_handler

# Configure fallback order
global_error_handler.fallback_configs["llm"].fallback_providers = [
    "openai_gpt",      # Primary
    "anthropic_claude", # Fallback 1 (if you have Anthropic API key)
    "local_llm"        # Fallback 2 (local model)
]

global_error_handler.fallback_configs["stt"].fallback_providers = [
    "openai_whisper",  # Primary  
    "azure_speech",    # Fallback 1 (if you have Azure API key)
    "google_speech"    # Fallback 2 (if you have Google API key)
]
```

## üìä Performance Monitoring

```python
from monitoring.performance_monitor import global_performance_monitor

# Start monitoring
await global_performance_monitor.start_monitoring()

# Register components
stt_monitor = global_performance_monitor.register_component("openai_stt", "stt")
llm_monitor = global_performance_monitor.register_component("openai_llm", "llm")

# Get performance metrics
async with llm_monitor.monitor_operation("generate_response"):
    response = await llm_provider.generate_response(messages)

# View metrics
summary = global_performance_monitor.get_overall_summary()
print(f"Average latency: {summary['components']['openai_llm']['avg_duration_ms']}ms")
```

## üîß Testing Your Setup

Run our validation script to test your OpenAI configuration:

```bash
# Test basic structure and API connectivity
python3 test_task1_structure.py

# Test with real OpenAI API (requires dependencies)
python3 test_openai_integration.py
```

## üìà Expected Performance

With this OpenAI-focused configuration, you should achieve:

- **STT Latency**: ~100-300ms (Whisper)
- **LLM Latency**: ~200-800ms (GPT-4.1-mini)  
- **Total Pipeline**: <236ms target (excluding TTS)
- **Accuracy**: 95%+ STT accuracy, high-quality LLM responses
- **Cost**: Significantly lower than GPT-4o while maintaining quality

## üåü Benefits of This Setup

‚úÖ **Cost-Effective**: GPT-4.1-mini is ~60% cheaper than GPT-4o
‚úÖ **High Performance**: Latest models with better capabilities  
‚úÖ **Minimal Dependencies**: Only need OpenAI API key for core functionality
‚úÖ **Production Ready**: Full error handling and monitoring
‚úÖ **Scalable**: Built-in fallback mechanisms for reliability

This configuration gives you a production-ready voice agent pipeline focused on OpenAI services while maintaining the flexibility to add other providers as needed.